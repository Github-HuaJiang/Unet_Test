library(terra)
library(doParallel)
library(foreach)
ROI <-st_read("D:/1_Bea_Ticino/ROI_30x30_rilievo_floristico_UTM_finale_CINZIA.shp")
forest <- st_read("D:/1_Bea_Ticino/border_forest_final2.shp")
ticino_dir_Cab<-file.path("D:/1_Bea_Ticino/Cab_rename")
sen2r_Ca_paths<-load_s2paths(ticino_dir_Cab,
time_window = c("2017-01-01", "2023-12-31"),
file_ext="tif$")
# prepare point vector file with one point per S2-pixel-center
# get one of the S2-scenes
dumimg <- rast(sen2r_Cab_paths[1])
sen2r_Cab_paths<-load_s2paths(ticino_dir_Cab,
time_window = c("2017-01-01", "2023-12-31"),
file_ext="tif$")
# prepare point vector file with one point per S2-pixel-center
# get one of the S2-scenes
dumimg <- rast(sen2r_Cab_paths[1])
dumimg2 <- crop(dumimg, ext(forest))
dumimg3 <- mask(dumimg2, forest)
plot(dumimg3)
# check structure
structure(dumimg2)
# get extent
imgex <- ext(dumimg2)
# get all x-cor of all pixel centers
xcor <- seq(imgex[1]+5, imgex[2]-5, 10)
# get all y-cor of all pixel centers
ycor <- seq(imgex[3]+5, imgex[4]-5, 10)
# get all x,y-cor combinations
xy <- expand.grid(x=xcor, y=ycor)
head(xy)
# work around to make the "vect" function work
x = xy[,1]
y = xy[,2]
#xy2 <- data.frame(x,y)
#xy2$id <- seq(1,nrow(xy2),1)
xy3 <- cbind(x,y)
# create spatial point vector file
all_pix_pts <- vect(xy3, crs=crs(ROI))
# extract values of masked image to identify forest pixels
forpixs <- extract(dumimg3,all_pix_pts)
table(forpixs[,2])
# subset spatvect points to those that are in forests
all_pix_pts_for <- subset(all_pix_pts, !is.na(forpixs[,2]))
# check how many pixels are left
length(all_pix_pts_for)
# extract x,y values of remaining pixels
xyfor <- geom(all_pix_pts_for)
head(xyfor)
xyfor2 <- as.data.frame(xyfor[,3:4])
fromvals <- seq(1, nrow(xyfor2), 100)
tovals <- fromvals[2:length(fromvals)]-1
head(fromvals)
head(tovals)
# Function to calculate standardized anomalies
calculate_standardized_anomaly <- function(raw_data, daily_averages_id,location_std) {
standardized_anomalies <- (raw_data - daily_averages_id) / location_std
return(standardized_anomalies)
}
tail(tovals)
cl <- makeCluster(70)
registerDoParallel(cl)
#for (i in 1:length(tovals)){
start.time <- Sys.time()
#for (i in 1:10){
foreach(i=20803:length(fromvals), .packages=c("terra","sen2r","sf","sen2rts","TDPanalysis","dplyr","lubridate","tidyr","terra")) %dopar% {
# get spatial point vector
all_pix_pts <- sf::st_as_sf(xyfor2[fromvals[i]:tovals[i],], coords = c("x", "y"), crs=terra::crs(ROI))
all_pix_pts$ID <- seq(fromvals[i],tovals[i],1)
# now apply Bea's workflow
ts_raw_Cab <- sen2rts::extract_s2ts(sen2r_Cab_paths, all_pix_pts,in_sf_id = "ID")
#date with clouds that makes peaks (to check if there are overs)
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2017-06-26", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2019-07-31", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2019-09-29", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2020-05-06", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2020-06-05", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2021-05-06", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2021-06-30", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2022-05-26", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2023-06-15", ]
#QUARTILI----Cab------apply quartile directly on SMOOTHING ------- END 349 START 46 ------ quartile 0.05
smooth_Cab <- sen2rts::smooth_s2ts(ts_raw_Cab,spike = 0.3, noise_dir = "low",sg_polynom = 2,
sg_daywindow = 30, max_extrapolation = 0.05)
#plot(smooth_Cab)
ts_smooth<-smooth_Cab
ts_smooth$year<-format(ts_smooth$date,"%Y")
ts_smooth$doy<-date.to.DOY(format(ts_smooth$date,"%d/%m/%Y"),format="dd/mm/yyyy")
ts_smooth$date<-as.Date(ts_smooth$date,"%Y-%m-%d")
pos_for<-unique(ts_smooth$id)
ts_smooth_quantfilled<-data.frame()
doy_winter_end<-46
doy_winter_start<-349
threshold<-0.05
for (f in 1:length(pos_for))
{ts_smooth_for<-ts_smooth[ts_smooth$id==pos_for[f],]
pos_year<-unique(ts_smooth_for$year)
quant_05<-quantile(ts_smooth_for$value,probs=threshold,na.rm=TRUE)
for(y in 1:length(pos_year))
{
ts_smooth_for_year<-ts_smooth_for[ts_smooth_for$year==pos_year[y],]
ts_smooth_for_year$value[ts_smooth_for_year$doy<=doy_winter_end | ts_smooth_for_year$doy>=doy_winter_start]<-quant_05
ts_smooth_quantfilled<-rbind(ts_smooth_quantfilled,ts_smooth_for_year)
}
}
ts_Cab_QUARTILI_smoothed<- ts_smooth_quantfilled
# formula per immettere file esterno nel formato del pachetto--- Cab
ts_Cab_QUARTILI_smoothed<-s2ts(id = ts_Cab_QUARTILI_smoothed$id,date =ts_Cab_QUARTILI_smoothed$date,
value = ts_Cab_QUARTILI_smoothed$value,
rawval= ts_Cab_QUARTILI_smoothed$rawval,
orbit=ts_Cab_QUARTILI_smoothed$orbit,
sensor=ts_Cab_QUARTILI_smoothed$sensor)
#save(ts_Cab_QUARTILI_smoothed, file = "ts_Cab_QUARTILI_smoothed.RData")
#FILLING -----Cab---------punto in qui salvo per fare anomalie
Fill_Cab_Q_base_new<- fill_s2ts(ts_Cab_QUARTILI_smoothed,frequency = "daily", max_na_days = Inf,
max_extrapolation = 0.1)
# Cab --- ANOMALIE ----------------------------------------------
#   DAILY MEAN BY LOCAtion
# step1 - CALCULATE ST ANOMALIES -multi_year daily-monthly averages by id----------------------------------------------
# Extract year and month from the date column
Fill_Cab_Q <- Fill_Cab_Q_base_new %>%
mutate(year = format(date, "%Y"),
month = format(date, "%m"),
day = format(date, "%d"))
daily_averages_id <- Fill_Cab_Q %>%
group_by(day,month,id) %>%
summarise(mean_value = mean(value, na.rm = TRUE))
# Merge the calculated daily averages back into the Fill_Cab_Q dataframe
Fill_Cab_Q_daily.avr_ID <- merge(Fill_Cab_Q, daily_averages_id, by = c("day", "month","id"), all.x = TRUE)
#article std
location_std <- Fill_Cab_Q_daily.avr_ID %>%
group_by(id) %>%
summarize(location.multiyear_std = sd(value, na.rm = TRUE)) %>%
ungroup()
Fill_Cab_Q_daily.avr_ID <- Fill_Cab_Q_daily.avr_ID %>%
left_join(location_std, by = "id")
# Calculate standardized anomalies using the function
Fill_Cab_Q_daily.avr_ID$std_anomaly <- calculate_standardized_anomaly(Fill_Cab_Q_daily.avr_ID$value, Fill_Cab_Q_daily.avr_ID$mean_value, Fill_Cab_Q_daily.avr_ID$location.multiyear_std)
#str(Fill_Cab_Q_daily.avr_ID)
# save standard anomalies ------------------------------------
csvname <- paste0("D:/1_Bea_Ticino/1_results_anomalies2/anomalies",fromvals[i],"_",tovals[i],".csv")
rdataname <- paste0("D:/1_Bea_Ticino/1_results_anomalies2/anomalies",fromvals[i],"_",tovals[i],".RData")
write.csv(Fill_Cab_Q_daily.avr_ID, csvname, row.names=FALSE)
save(Fill_Cab_Q_daily.avr_ID, file = rdataname)
print(i)
}
stopCluster(cl)
stopCluster(cl)
cl <- makeCluster(70)
registerDoParallel(cl)
#for (i in 1:length(tovals)){
start.time <- Sys.time()
#for (i in 1:10){
foreach(i=1:20803, .packages=c("terra","sen2r","sf","sen2rts","TDPanalysis","dplyr","lubridate","tidyr","terra")) %dopar% {
# get spatial point vector
all_pix_pts <- sf::st_as_sf(xyfor2[fromvals[i]:tovals[i],], coords = c("x", "y"), crs=terra::crs(ROI))
all_pix_pts$ID <- seq(fromvals[i],tovals[i],1)
# now apply Bea's workflow
ts_raw_Cab <- sen2rts::extract_s2ts(sen2r_Cab_paths, all_pix_pts,in_sf_id = "ID")
#date with clouds that makes peaks (to check if there are overs)
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2017-06-26", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2019-07-31", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2019-09-29", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2020-05-06", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2020-06-05", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2021-05-06", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2021-06-30", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2022-05-26", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2023-06-15", ]
#QUARTILI----Cab------apply quartile directly on SMOOTHING ------- END 349 START 46 ------ quartile 0.05
smooth_Cab <- sen2rts::smooth_s2ts(ts_raw_Cab,spike = 0.3, noise_dir = "low",sg_polynom = 2,
sg_daywindow = 30, max_extrapolation = 0.05)
#plot(smooth_Cab)
ts_smooth<-smooth_Cab
ts_smooth$year<-format(ts_smooth$date,"%Y")
ts_smooth$doy<-date.to.DOY(format(ts_smooth$date,"%d/%m/%Y"),format="dd/mm/yyyy")
ts_smooth$date<-as.Date(ts_smooth$date,"%Y-%m-%d")
pos_for<-unique(ts_smooth$id)
ts_smooth_quantfilled<-data.frame()
doy_winter_end<-46
doy_winter_start<-349
threshold<-0.05
for (f in 1:length(pos_for))
{ts_smooth_for<-ts_smooth[ts_smooth$id==pos_for[f],]
pos_year<-unique(ts_smooth_for$year)
quant_05<-quantile(ts_smooth_for$value,probs=threshold,na.rm=TRUE)
for(y in 1:length(pos_year))
{
ts_smooth_for_year<-ts_smooth_for[ts_smooth_for$year==pos_year[y],]
ts_smooth_for_year$value[ts_smooth_for_year$doy<=doy_winter_end | ts_smooth_for_year$doy>=doy_winter_start]<-quant_05
ts_smooth_quantfilled<-rbind(ts_smooth_quantfilled,ts_smooth_for_year)
}
}
ts_Cab_QUARTILI_smoothed<- ts_smooth_quantfilled
# formula per immettere file esterno nel formato del pachetto--- Cab
ts_Cab_QUARTILI_smoothed<-s2ts(id = ts_Cab_QUARTILI_smoothed$id,date =ts_Cab_QUARTILI_smoothed$date,
value = ts_Cab_QUARTILI_smoothed$value,
rawval= ts_Cab_QUARTILI_smoothed$rawval,
orbit=ts_Cab_QUARTILI_smoothed$orbit,
sensor=ts_Cab_QUARTILI_smoothed$sensor)
#save(ts_Cab_QUARTILI_smoothed, file = "ts_Cab_QUARTILI_smoothed.RData")
#FILLING -----Cab---------punto in qui salvo per fare anomalie
Fill_Cab_Q_base_new<- fill_s2ts(ts_Cab_QUARTILI_smoothed,frequency = "daily", max_na_days = Inf,
max_extrapolation = 0.1)
# Cab --- ANOMALIE ----------------------------------------------
#   DAILY MEAN BY LOCAtion
# step1 - CALCULATE ST ANOMALIES -multi_year daily-monthly averages by id----------------------------------------------
# Extract year and month from the date column
Fill_Cab_Q <- Fill_Cab_Q_base_new %>%
mutate(year = format(date, "%Y"),
month = format(date, "%m"),
day = format(date, "%d"))
daily_averages_id <- Fill_Cab_Q %>%
group_by(day,month,id) %>%
summarise(mean_value = mean(value, na.rm = TRUE))
# Merge the calculated daily averages back into the Fill_Cab_Q dataframe
Fill_Cab_Q_daily.avr_ID <- merge(Fill_Cab_Q, daily_averages_id, by = c("day", "month","id"), all.x = TRUE)
#article std
location_std <- Fill_Cab_Q_daily.avr_ID %>%
group_by(id) %>%
summarize(location.multiyear_std = sd(value, na.rm = TRUE)) %>%
ungroup()
Fill_Cab_Q_daily.avr_ID <- Fill_Cab_Q_daily.avr_ID %>%
left_join(location_std, by = "id")
# Calculate standardized anomalies using the function
Fill_Cab_Q_daily.avr_ID$std_anomaly <- calculate_standardized_anomaly(Fill_Cab_Q_daily.avr_ID$value, Fill_Cab_Q_daily.avr_ID$mean_value, Fill_Cab_Q_daily.avr_ID$location.multiyear_std)
#str(Fill_Cab_Q_daily.avr_ID)
# save standard anomalies ------------------------------------
csvname <- paste0("D:/1_Bea_Ticino/1_results_anomalies2/anomalies",fromvals[i],"_",tovals[i],".csv")
rdataname <- paste0("D:/1_Bea_Ticino/1_results_anomalies2/anomalies",fromvals[i],"_",tovals[i],".RData")
write.csv(Fill_Cab_Q_daily.avr_ID, csvname, row.names=FALSE)
save(Fill_Cab_Q_daily.avr_ID, file = rdataname)
print(i)
}
end.time <- Sys.time()
end.time-start.time
# get spatial point vector
#all_pix_pts <- sf::st_as_sf(xyfor2[fromvals[i]:tovals[i],], coords = c("x", "y"), crs=terra::crs(ROI))
#all_pix_pts$ID <- seq(fromvals[i],tovals[i],1)
all_pix_pts <- sf::st_as_sf(xyfor2[2134501:2134561,], coords = c("x", "y"), crs=terra::crs(ROI))
all_pix_pts$ID <- seq(2134501,2134561,1)
ts_raw_Cab <- sen2rts::extract_s2ts(sen2r_Cab_paths, all_pix_pts,in_sf_id = "ID")
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2017-06-26", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2019-07-31", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2019-09-29", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2020-05-06", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2020-06-05", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2021-05-06", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2021-06-30", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2022-05-26", ]
ts_raw_Cab <- ts_raw_Cab[!ts_raw_Cab$date == "2023-06-15", ]
#QUARTILI----Cab------apply quartile directly on SMOOTHING ------- END 349 START 46 ------ quartile 0.05
smooth_Cab <- sen2rts::smooth_s2ts(ts_raw_Cab,spike = 0.3, noise_dir = "low",sg_polynom = 2,
sg_daywindow = 30, max_extrapolation = 0.05)
#plot(smooth_Cab)
ts_smooth<-smooth_Cab
ts_smooth$year<-format(ts_smooth$date,"%Y")
ts_smooth$doy<-date.to.DOY(format(ts_smooth$date,"%d/%m/%Y"),format="dd/mm/yyyy")
ts_smooth$date<-as.Date(ts_smooth$date,"%Y-%m-%d")
pos_for<-unique(ts_smooth$id)
ts_smooth_quantfilled<-data.frame()
doy_winter_end<-46
doy_winter_start<-349
threshold<-0.05
for (f in 1:length(pos_for))
{ts_smooth_for<-ts_smooth[ts_smooth$id==pos_for[f],]
pos_year<-unique(ts_smooth_for$year)
quant_05<-quantile(ts_smooth_for$value,probs=threshold,na.rm=TRUE)
for(y in 1:length(pos_year))
{
ts_smooth_for_year<-ts_smooth_for[ts_smooth_for$year==pos_year[y],]
ts_smooth_for_year$value[ts_smooth_for_year$doy<=doy_winter_end | ts_smooth_for_year$doy>=doy_winter_start]<-quant_05
ts_smooth_quantfilled<-rbind(ts_smooth_quantfilled,ts_smooth_for_year)
}
}
ts_Cab_QUARTILI_smoothed<- ts_smooth_quantfilled
# formula per immettere file esterno nel formato del pachetto--- Cab
ts_Cab_QUARTILI_smoothed<-s2ts(id = ts_Cab_QUARTILI_smoothed$id,date =ts_Cab_QUARTILI_smoothed$date,
value = ts_Cab_QUARTILI_smoothed$value,
rawval= ts_Cab_QUARTILI_smoothed$rawval,
orbit=ts_Cab_QUARTILI_smoothed$orbit,
sensor=ts_Cab_QUARTILI_smoothed$sensor)
#FILLING -----Cab---------punto in qui salvo per fare anomalie
Fill_Cab_Q_base_new<- fill_s2ts(ts_Cab_QUARTILI_smoothed,frequency = "daily", max_na_days = Inf,
max_extrapolation = 0.1)
# Extract year and month from the date column
Fill_Cab_Q <- Fill_Cab_Q_base_new %>%
mutate(year = format(date, "%Y"),
month = format(date, "%m"),
day = format(date, "%d"))
daily_averages_id <- Fill_Cab_Q %>%
group_by(day,month,id) %>%
summarise(mean_value = mean(value, na.rm = TRUE))
# Merge the calculated daily averages back into the Fill_Cab_Q dataframe
Fill_Cab_Q_daily.avr_ID <- merge(Fill_Cab_Q, daily_averages_id, by = c("day", "month","id"), all.x = TRUE)
#article std
location_std <- Fill_Cab_Q_daily.avr_ID %>%
group_by(id) %>%
summarize(location.multiyear_std = sd(value, na.rm = TRUE)) %>%
ungroup()
Fill_Cab_Q_daily.avr_ID <- Fill_Cab_Q_daily.avr_ID %>%
left_join(location_std, by = "id")
# Calculate standardized anomalies using the function
Fill_Cab_Q_daily.avr_ID$std_anomaly <- calculate_standardized_anomaly(Fill_Cab_Q_daily.avr_ID$value, Fill_Cab_Q_daily.avr_ID$mean_value, Fill_Cab_Q_daily.avr_ID$location.multiyear_std)
# save standard anomalies ------------------------------------
#csvname <- paste0("D:/1_Bea_Ticino/1_results_anomalies2/anomalies",fromvals[i],"_",tovals[i],".csv")
#rdataname <- paste0("D:/1_Bea_Ticino/1_results_anomalies2/anomalies",fromvals[i],"_",tovals[i],".RData")
csvname <- paste0("D:/1_Bea_Ticino/1_results_anomalies2/anomalies2134501_2134561.csv")
rdataname <- paste0("D:/1_Bea_Ticino/1_results_anomalies2/anomalies2134501_2134561.RData")
write.csv(Fill_Cab_Q_daily.avr_ID, csvname, row.names=FALSE)
save(Fill_Cab_Q_daily.avr_ID, file = rdataname)
library(sen2r)
library(sf)
library(sen2rts)
library(ggplot2)
library(TDPanalysis)
library(dplyr)
library(lubridate)
library(tidyr)
library(terra)
library(doParallel)
library(foreach)
ROI <-st_read("D:/1_Bea_Ticino/ROI_30x30_rilievo_floristico_UTM_finale_CINZIA.shp")
forest <- st_read("D:/1_Bea_Ticino/border_forest_final2.shp")
ticino_dir_LAI<-file.path("E:/00_FUB_HP/98_Forschung/28_Beatrice/1_Bea_Ticino/LAI_rename")
ticino_dir_LAI<-file.path("D:/1_Bea_Ticino/Cab_rename")
sen2r_LAI_paths<-load_s2paths(ticino_dir_LAI,
time_window = c("2017-01-01", "2023-12-31"),
file_ext="tif$")
# prepare point vector file with one point per S2-pixel-center
# get one of the S2-scenes
dumimg <- rast(sen2r_LAI_paths[1])
dumimg2 <- crop(dumimg, ext(forest))
dumimg3 <- mask(dumimg2, forest)
plot(dumimg3)
# check structure
structure(dumimg2)
# get extent
imgex <- ext(dumimg2)
# get all x-cor of all pixel centers
xcor <- seq(imgex[1]+5, imgex[2]-5, 10)
# get all y-cor of all pixel centers
ycor <- seq(imgex[3]+5, imgex[4]-5, 10)
# get all x,y-cor combinations
xy <- expand.grid(x=xcor, y=ycor)
head(xy)
# work around to make the "vect" function work
x = xy[,1]
y = xy[,2]
#xy2 <- data.frame(x,y)
#xy2$id <- seq(1,nrow(xy2),1)
xy3 <- cbind(x,y)
# create spatial point vector file
all_pix_pts <- vect(xy3, crs=crs(ROI))
# extract values of masked image to identify forest pixels
forpixs <- extract(dumimg3,all_pix_pts)
head(forpixs)
#table(forpixs[,2])
# subset spatvect points to those that are in forests
all_pix_pts_for <- subset(all_pix_pts, !is.na(forpixs[,2]))
# check how many pixels are left
length(all_pix_pts_for)
# get anomaly filenames
anomfils <- list.files("D:/1_Bea_Ticino/1_results_anomalies2_Cab", full.names = T, pattern=".RData")
# prepare loops by getting unique dates
load(anomfils[1])
date <- Fill_Cab_Q_daily.avr_ID$date
dates_avail <- sort(unique(date))
str(Fill_Cab_Q_daily.avr_ID)
# loop through anomaly files and extract relevant information
weekly_dates <- dates_avail[seq(1,7,length(dates_avail))]
weekly_dates
dates_avail
seq(1,7,length(dates_avail))
length(dates_avail)
weekly_dates <- dates_avail[seq(1,length(dates_avail),7)]
weekly_dates
weekly_dates <- dates_avail[seq(1,length(dates_avail),7)]
cl <- makeCluster(20)
registerDoParallel(cl)
foreach(i2=1:length(weekly_dates), .packages=c("terra")) %dopar% {
map_for_day <- list()
dummy <- weekly_dates[i2]
for (i in 1:length(anomfils)){
#for (i in 1:1000){
load(anomfils[i])
date_it <- Fill_Cab_Q_daily.avr_ID$date
anom_it <- Fill_Cab_Q_daily.avr_ID$std_anomaly
pix_id <- Fill_Cab_Q_daily.avr_ID$id
date_anom <- data.frame(date_it, anom_it, pix_id)
anom_date <- date_anom[date_anom$date_it == dummy,]
map_for_day[[i]] <- anom_date
print(i)
}
anom_out <- do.call(rbind, map_for_day)
nameout <- paste0("anomaly_map_Cab_",  weekly_dates[i2], ".RData")
save(anom_out, file=nameout)
print (i2)
}
library(sen2r)
library(sf)
library(sen2rts)
library(ggplot2)
library(TDPanalysis)
library(dplyr)
library(lubridate)
library(tidyr)
library(terra)
library(doParallel)
library(foreach)
ROI <-st_read("D:/1_Bea_Ticino/ROI_30x30_rilievo_floristico_UTM_finale_CINZIA.shp")
forest <- st_read("D:/1_Bea_Ticino/border_forest_final2.shp")
ticino_dir_LAI<-file.path("D:/1_Bea_Ticino/LAI_rename")
sen2r_LAI_paths<-load_s2paths(ticino_dir_LAI,
time_window = c("2017-01-01", "2023-12-31"),
file_ext="tif$")
# prepare point vector file with one point per S2-pixel-center
# get one of the S2-scenes
dumimg <- rast(sen2r_LAI_paths[1])
dumimg2 <- crop(dumimg, ext(forest))
dumimg3 <- mask(dumimg2, forest)
plot(dumimg3)
# check structure
structure(dumimg2)
# get extent
imgex <- ext(dumimg2)
# get all x-cor of all pixel centers
xcor <- seq(imgex[1]+5, imgex[2]-5, 10)
# get all y-cor of all pixel centers
ycor <- seq(imgex[3]+5, imgex[4]-5, 10)
# get all x,y-cor combinations
xy <- expand.grid(x=xcor, y=ycor)
head(xy)
# work around to make the "vect" function work
x = xy[,1]
y = xy[,2]
#xy2 <- data.frame(x,y)
#xy2$id <- seq(1,nrow(xy2),1)
xy3 <- cbind(x,y)
# create spatial point vector file
all_pix_pts <- vect(xy3, crs=crs(ROI))
# extract values of masked image to identify forest pixels
forpixs <- extract(dumimg3,all_pix_pts)
head(forpixs)
# subset spatvect points to those that are in forests
all_pix_pts_for <- subset(all_pix_pts, !is.na(forpixs[,2]))
# check how many pixels are left
length(all_pix_pts_for)
# get anomaly filenames
anomfils <- list.files("D:/1_Bea_Ticino/1_results_anomalies1_LAI", full.names = T, pattern=".RData")
# prepare loops by getting unique dates
load(anomfils[1])
date <- Fill_LAI_Q_daily.avr_ID$date
date
dates_avail <- sort(unique(date))
dates_avail
weekly_dates <- dates_avail[seq(1,length(dates_avail),7)]
weekly_dates
Fill_LAI_Q_daily.avr_ID$std_anomaly
Fill_LAI_Q_daily.avr_ID$id
cl <- makeCluster(60)
registerDoParallel(cl)
foreach(i2=1:length(weekly_dates), .packages=c("terra")) %dopar% {
map_for_day <- list()
dummy <- weekly_dates[i2]
for (i in 1:length(anomfils)){
#for (i in 1:1000){
load(anomfils[i])
date_it <- Fill_LAI_Q_daily.avr_ID$date
anom_it <- Fill_LAI_Q_daily.avr_ID$std_anomaly
pix_id <- Fill_LAI_Q_daily.avr_ID$id
date_anom <- data.frame(date_it, anom_it, pix_id)
anom_date <- date_anom[date_anom$date_it == dummy,]
map_for_day[[i]] <- anom_date
print(i)
}
anom_out <- do.call(rbind, map_for_day)
nameout <- paste0("anomaly_map_LAI_",  weekly_dates[i2], ".RData")
save(anom_out, file=nameout)
print (i2)
}
require(terra)
masks <- list.files("D:/0_Tutorial/5_predict_comp/mask", pattern=".tif$", full.names = T)
origs <- list.files("D:/0_Tutorial/5_predict_comp", pattern=".tif$", full.names = T)
preds <- list.files("D:/0_Tutorial/6_model_comparison/model_20", pattern=".tif$", full.names = T)
imgs <- origs[1:10]
setwd("D:/0_Tutorial/6_model_comparison")
for (i in 1:10){
name <- paste0("model_20_", i, "_test.png")
png(file = name, width=2000, height=700)
par(mfrow=c(1,3))
plotRGB(rast(origs[[i]]), r=1, g=2, b=3, stretch="hist")
plot(rast(masks[i]))
plot(rast(preds[i]))
dev.off()
}
require(terra)
masks <- list.files("D:/0_Tutorial/5_predict_comp/mask", pattern=".tif$", full.names = T)
origs <- list.files("D:/0_Tutorial/5_predict_comp", pattern=".tif$", full.names = T)
preds <- list.files("D:/0_Tutorial/6_model_comparison/model_21", pattern=".tif$", full.names = T)
imgs <- origs[1:10]
setwd("D:/0_Tutorial/6_model_comparison")
for (i in 1:10){
name <- paste0("model_21_", i, "_test.png")
png(file = name, width=2000, height=700)
par(mfrow=c(1,3))
plotRGB(rast(origs[[i]]), r=1, g=2, b=3, stretch="hist")
plot(rast(masks[i]))
plot(rast(preds[i]))
dev.off()
}
